{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# From Statistics to Machine Learning [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rndsrc/stat2ml/blob/main/stat2ml.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "![meme](fig/stat2ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Why AI?\n",
    "\n",
    "AI is everywhere in the news these days.\n",
    "\n",
    "People debate whether we are in an AI bubble, there is even a\n",
    "[Wikipedia page about it](https://en.wikipedia.org/wiki/AI_bubble),\n",
    "and whether today's massive investments will lead to transformative\n",
    "breakthroughs or another AI winter.\n",
    "\n",
    "But regardless of future speculation, one thing is clear:\n",
    "\n",
    "> **AI has already delivered real, undeniable scientific and\n",
    ">   technological breakthroughs.**\n",
    "\n",
    "Students use ChatGPT daily, but more importantly, the last decade has\n",
    "produced advances that matter directly to science:\n",
    "* **AlphaGo** (2016):\n",
    "  A [reinforcement learning system](https://www.nature.com/articles/nature16961)\n",
    "  that defeated the world champion in Go—a game once believed to\n",
    "  require human intuition.\n",
    "  There is even a\n",
    "  [documentory](https://www.youtube.com/watch?v=WXuK6gekU1Y) on it.\n",
    "* **The Transformer Architecture** (2017):\n",
    "  Introduced in\n",
    "  \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\",\n",
    "  this model revolutionized sequence learning and laid the foundation\n",
    "  for today's large language models including ChatGPT.\n",
    "* **AlphaFold** (~2020):\n",
    "  Achieved near-experimental accuracy in\n",
    "  [predicting protein structures](https://www.nature.com/articles/s41586-021-03819-2),\n",
    "  solving a 50-year grand challenge in biology, and won the\n",
    "  [Nobel Prize in Chemistry 2024](https://www.nobelprize.org/prizes/chemistry/2024/summary/).\n",
    "* **Large Language Models** (~2020-today):\n",
    "  complex reasoning, coding, symbolic manipulation, and\n",
    "  scientific workflows at scale.\n",
    "  Notable startups include\n",
    "  [OpenAI](https://openai.com/) and\n",
    "  [Anthropic](https://www.anthropic.com/).\n",
    "* **Diffusion Models** (~2020-today):\n",
    "  Generated photorealistic images, molecular structures, and\n",
    "  simulation surrogates using\n",
    "  [probabilistic forward-reverse processes](https://arxiv.org/abs/2209.00796).\n",
    "* **AI-assisted scientific discovery** (today):\n",
    "  [AI systems](https://www.nature.com/articles/d41586-024-02842-3)\n",
    "  now help design new materials, discover antibiotics,\n",
    "  control fusion plasmas, and analyze particle-physics, astrophysics,\n",
    "  and cosmology datasets.\n",
    "\n",
    "There are true algorithmic innovations and real scientific value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "As physicists and scientists, it is important to ask:\n",
    "\n",
    "> **What is AI?  \n",
    ">   How does AI work?  \n",
    ">   How can we use AI to accelerate scientific discovery?**\n",
    "\n",
    "The last question can become an excellent open-ended homework\n",
    "problem.\n",
    "For this lab, we will focus primarily on the first two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## What is AI/ML?\n",
    "\n",
    "The current wave of AI can be viewed as a continuation of earlier\n",
    "ideas that appeared under buzz words like *\"big data\"* and *\"data\n",
    "science\"*,\n",
    "\n",
    "To understand this evolution, it helps to take a step back and look at\n",
    "the history of scientific methodology.\n",
    "\n",
    "### The First Two Paradigms: Experiment & Theory\n",
    "\n",
    "Before modern computing, science operated through two complementary\n",
    "paradigms:\n",
    "\n",
    "1. **Empirical/Experimental Science**\n",
    "\n",
    "   * Start with observations.\n",
    "   * Identify patterns and regularities in nature.\n",
    "   * Build phenomenological descriptions.\n",
    "\n",
    "2. **Theoretical Science**\n",
    "\n",
    "   * Start with mathematical principles.\n",
    "   * Derive predictions about how systems should behave.\n",
    "   * Compare theory to experiments.\n",
    "\n",
    "Together, experiment and theory form the foundation of the classical\n",
    "scientific method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "3.  **The Third Pillar: Computational Science**\n",
    "\n",
    "    As physics, chemistry, and engineering advanced, systems became\n",
    "    too complex for purely pencil-and-paper analysis: turbulence,\n",
    "    weather, plasma physics, galaxy formation, quantum many-body\n",
    "    systems, general relativity, etc.\n",
    "\n",
    "    Numerical algorithms became essential:\n",
    "\n",
    "    > Theoretical science + computing power = computational science.\n",
    "\n",
    "    This gave rise to the **third pillar: computational science**,\n",
    "    i.e., using algorithms and simulations to test and extend theory,\n",
    "    and make predictions.\n",
    "\n",
    "    ![The Third Pillar](fig/third.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "4.  The Fourth Paradigm: Data Science\n",
    "\n",
    "    In recent decades:\n",
    "    * Experiments produce massive data streams (radio telescopes,\n",
    "      climate satellites).\n",
    "    * Sensors became cheap and popular.\n",
    "    * Digital transactions and interactions created enormous datasets.\n",
    "\n",
    "    When **the data** becomes too large for traditional statistical\n",
    "    analysis, we need new tools to find structure, correlations, and\n",
    "    predictions.\n",
    "\n",
    "    This drove the \"big data\" era and the rise of **data science**:\n",
    "\n",
    "    > Empirical science + computing power = data science.\n",
    "\n",
    "    This is the **fourth paradigm of science**.\n",
    "\n",
    "    ![The Fourth Paradigm](fig/fourth.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Machine Learning: Let the Computer Learn the Pattern\n",
    "\n",
    "One natural consequence of data science is that the algorithms we\n",
    "build often become *useful beyond just summarizing data*.\n",
    "For example, an algorithm that characterizes pixel patterns in images\n",
    "can also be used to recognize digits, classify galaxies, or identify\n",
    "particle tracks.\n",
    "In other words:\n",
    "\n",
    "> Instead of manually writing a program to perform a task, we can\n",
    "> train a model to *learn* how to perform the task directly from data.\n",
    "\n",
    "If we have enough examples and a suitable model, the computer can\n",
    "infer the mapping between inputs and outputs on its own.\n",
    "This idea of letting the computer learn patterns, rules, or behaviors\n",
    "from data is the essence of **machine learning (ML)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### What About AI?\n",
    "\n",
    "The term **Artificial Intelligence (AI)** has a long, complicated, and\n",
    "sometimes\n",
    "[hype-filled history](https://en.wikipedia.org/wiki/History_of_artificial_intelligence).\n",
    "Today, in practice:\n",
    "\n",
    "* **ML** often refers to the specific algorithms and mathematical\n",
    "  tools that learn from data.\n",
    "  These include linear models, neural networks, decision trees,\n",
    "  reinforcement learning, etc.\n",
    "* **AI** is often used as a broad umbrella term, or sometimes a\n",
    "  marketing term, for systems *powered by* machine learning.\n",
    "* When an ML system becomes extremely capable like ChatGPT, AlphaFold,\n",
    "  AlphaGo, or a self-driving car, we tend to call it **AI**,\n",
    "  especially outside technical circles.\n",
    "\n",
    "A useful rule of thumb for modern usage:\n",
    "\n",
    "> **ML is the toolbox.  \n",
    ">   AI is what we call ML systems when they look impressive,  \n",
    ">   or when we want other people to think they are impressive.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## The Machine Learning Landscape\n",
    "\n",
    "ML is not just statistics.\n",
    "In fact the above meme may upset a lot of people.\n",
    "However, a large portion of modern ML is deeply grounded in\n",
    "**statistical reasoning**, **probability**, and **optimization**.\n",
    "\n",
    "To understand where our lab fits, it helps to map the landscape at a\n",
    "high level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "In **supervised learning**, we are given input–output pairs $(x, y)$:\n",
    "\n",
    "* $x$: the data\n",
    "* $y$: the label or target\n",
    "* The task is to learn a function $f_\\theta(x) \\approx y$, where the\n",
    "  parameters $\\theta$ are adjusted using examples.\n",
    "\n",
    "Because each input comes with a **label**, supervised learning has a\n",
    "clear objective: make predictions that match the provided examples.\n",
    "\n",
    "This makes supervised learning fundamentally similar to **curve\n",
    "fitting**:\n",
    "\n",
    "> We choose a function with adjustable parameters and fit it to\n",
    "> labeled data.\n",
    "\n",
    "Whether the function is a line, a polynomial, or a giant neural\n",
    "network, the principle is the same.\n",
    "For this reason, our lab focuses on supervised learning.\n",
    "It provides the cleanest conceptual connection between statistics and\n",
    "modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "In contrast, **unsupervised learning** provides only the inputs $x$,\n",
    "with no labels.\n",
    "The goal is to find structure or patterns *without* being told what\n",
    "the correct output should be.\n",
    "\n",
    "However, unsupervised learning is *not* a single coherent class of\n",
    "algorithms.\n",
    "Instead, it is a loosely grouped collection of very different\n",
    "techniques, such as:\n",
    "\n",
    "* **PCA**: finds directions of maximum variance\n",
    "* **$k$-means**: partitions points into clusters\n",
    "* **Gaussian mixture models**: probabilistic clustering\n",
    "* **Autoencoders**: neural-network-based dimensionality reduction\n",
    "* **Density estimation**: learning probability distributions\n",
    "* **Manifold learning**: discovering low-dimensional structure\n",
    "\n",
    "These methods behave differently, solve different problems, and rely\n",
    "on different mathematical ideas.\n",
    "What unifies them is *only one thing*:\n",
    "\n",
    "> They all learn patterns from data **without labels**.\n",
    "\n",
    "Because unsupervised learning covers such a diverse set of tools and\n",
    "has no direct analog to curve fitting, we will not cover it in this\n",
    "introductory lab.\n",
    "A good place to see many popular unsupervised learning algorithm is\n",
    "[scikit-learn](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## What Is a Neural Network?\n",
    "\n",
    "We chose a function $f_\\theta(x)$ with parameters $\\theta$, defined a\n",
    "loss (e.g. $\\chi^2$ or MSE), and used optimization to find parameters\n",
    "that fit the data.\n",
    "A **neural network** is just a **very flexible family of functions**\n",
    "built by stacking simple building blocks:\n",
    "\n",
    "1. **Linear layer**  \n",
    "   $$\n",
    "   z = W x + b\n",
    "   $$\n",
    "   where\n",
    "   * $x$ is an input vector,\n",
    "   * $W$ is a matrix of weights,\n",
    "   * $b$ is a bias vector.\n",
    "\n",
    "2. **Nonlinear activation**  \n",
    "   Apply a nonlinear function element-wise, e.g. $\\tanh(z)$ or ReLU:\n",
    "   $$\n",
    "   h = \\tanh(z) \\quad \\text{or} \\quad h = \\max(0, z).\n",
    "   $$\n",
    "\n",
    "By **composing** several of these layers, we get a function like\n",
    "$$\n",
    "x \\;\\xrightarrow{\\text{linear + nonlinearity}}\\; h_1\n",
    "  \\;\\xrightarrow{\\text{linear + nonlinearity}}\\; h_2\n",
    "  \\;\\xrightarrow{\\text{linear + nonlinearity}}\\; ...\n",
    "  \\;\\xrightarrow{\\text{linear + nonlinearity}}\\; h_n\n",
    "  \\;\\xrightarrow{\\text{linear}}\\; \\text{logits} \\;\\xrightarrow{\\text{softmax}}\\; \\text{class probabilities}.\n",
    "$$\n",
    "\n",
    "This is called a **Multi-Layer Perceptron (MLP)** or fully-connected\n",
    "neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Our Model for MNIST\n",
    "\n",
    "In this lab, our network will:\n",
    "* take a **28×28 = 784-dimensional input vector** (flattened image),\n",
    "* pass it through multiple **hidden layers** with nonlinear activations,\n",
    "* output **10 numbers** (logits), one per digit class (0–9).\n",
    "\n",
    "Conceptually:\n",
    "$$\n",
    "\\text{image} \\in \\mathbb{R}^{784}\n",
    "\\;\\longrightarrow\\; h_1\n",
    "\\;\\longrightarrow\\; h_2\n",
    "\\;\\longrightarrow\\; ...\n",
    "\\;\\longrightarrow\\; h_n\n",
    "\\;\\longrightarrow\\; \\text{logits} \\in \\mathbb{R}^{10}\n",
    "\\;\\xrightarrow{\\text{softmax}}\\;\n",
    "\\text{probabilities over 10 digits}.\n",
    "$$\n",
    "\n",
    "This is still **curve fitting**, just in a **much higher-dimensional**\n",
    "and **more expressive** function space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### How Do We Train It?\n",
    "\n",
    "The training loop follows exactly the same pattern as before:\n",
    "\n",
    "1.  **Model**  \n",
    "    Our neural network $f_\\theta(x)$ with parameters $\\theta =\n",
    "    \\{W_\\ell, b_\\ell\\}$ for each layer.\n",
    "\n",
    "2.  **Loss function**  \n",
    "    We use **negative log-likelihood** (cross-entropy) on the\n",
    "    **log-softmax outputs**, which plays the same role as $\\chi^2$\n",
    "    did for regression.\n",
    "\n",
    "3.  **Optimization**  \n",
    "    We use **stochastic gradient descent (SGD)**:\n",
    "    * pick a mini-batch of labeled images,\n",
    "    * compute the gradient $\\nabla_\\theta \\mathcal{L}$,\n",
    "    * update $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}$.\n",
    "\n",
    "4.  **Autodiff with JAX**  \n",
    "    Instead of deriving gradients by hand, we let **JAX**\n",
    "    automatically differentiate the loss with respect to all\n",
    "    parameters.\n",
    "\n",
    "So, when you see the code for:\n",
    "\n",
    "* `init_params` -> choosing an initial guess for $\\theta$\n",
    "* `predict` -> the function $f_\\theta(x)$\n",
    "* `loss` -> the scalar loss $\\mathcal{L}(\\theta)$\n",
    "* `update` -> one SGD step using `grad(loss)`\n",
    "\n",
    "you should think:\n",
    "\n",
    "> **This is the same curve-fitting story as we did in undergrad lab,\n",
    ">   just with a bigger function and automatic gradients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Neural Network with JAX\n",
    "\n",
    "In this lab, we will:\n",
    "1. Downloads and loads MNIST into NumPy arrays (if it doesn't already\n",
    "   exist locally).\n",
    "2. Builds a simple Multi-Layer Perceptron in JAX.\n",
    "3. Trains the network on MNIST.\n",
    "4. Evaluates the performance on test data.\n",
    "5. Provides a custom inference function for your own handwriting\n",
    "   images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This lab is based on a\n",
    "[JAX example](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier_fromscratch.py).\n",
    "\n",
    "Please notice that MNIST is the \"hello world\" for machine learning,\n",
    "and there are many many examples available online, including some\n",
    "simplier ones that use libraries:\n",
    "[JAX with pre-built optimizers](https://github.com/jax-ml/jax/blob/main/examples/mnist_classifier.py),\n",
    "[FLAX](https://flax.readthedocs.io/en/latest/mnist_tutorial.html),\n",
    "[pytorch](https://github.com/pytorch/examples/tree/main/mnist), and\n",
    "[Keras](https://www.tensorflow.org/datasets/keras_example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: We would like to do some comparison ...\n",
    "#          Let's separate the class into three groups.\n",
    "#          Group 1 will use CPU for running this notebook.\n",
    "#          Groups 2 and 3 will use GPU and TPU, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### MNIST Data Loader\n",
    "\n",
    "We start by downloading the MNIST data set and store it locally.\n",
    "Our data loader will parse, reshape, normalize them, and return them\n",
    "in NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\":  \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\":  \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "for key, file in files.items():\n",
    "    if not isfile(file):\n",
    "        url = base_url + file\n",
    "        print(f\"Downloading {url} to {file}...\")\n",
    "        urlretrieve(url, file)\n",
    "    else:\n",
    "        print(f\"{file} exists; skip download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import array\n",
    "from jax import numpy as np\n",
    "\n",
    "# Parsing functions\n",
    "\n",
    "def parse_labels(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
    "        # Read the label data as 1-byte unsigned integers\n",
    "        return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "\n",
    "def parse_images(file):\n",
    "    with gzip.open(file, \"rb\") as fh:\n",
    "        _magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "        # Read the image data as 1-byte unsigned integers\n",
    "        images = np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
    "        # Reshape to (num_data, 28, 28)\n",
    "        images = images.reshape(num_data, rows, cols)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse raw data\n",
    "\n",
    "train_images_raw = parse_images(files[\"train_images\"])\n",
    "train_labels_raw = parse_labels(files[\"train_labels\"])\n",
    "\n",
    "test_images_raw  = parse_images(files[\"test_images\"])\n",
    "test_labels_raw  = parse_labels(files[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the images, i.e., flatten and normalize images to [0, 1]\n",
    "def standardize(images):\n",
    "    return images.reshape(-1, 28*28).astype(np.float32) / 255\n",
    "\n",
    "train_images = standardize(train_images_raw)\n",
    "test_images  = standardize(test_images_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "def one_hot(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels = one_hot(train_labels_raw, 10).astype(np.float32)\n",
    "test_labels  = one_hot(test_labels_raw,  10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Visualize Some Training and Testing Data\n",
    "\n",
    "Let's take a look at our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(3,6))\n",
    "ax0.imshow(train_images_raw[0,:,:], cmap='gray')\n",
    "ax1.imshow(test_images_raw[0,:,:],  cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Define a Simple Neural Network in JAX\n",
    "\n",
    "In this subsection, we introduce the core function needed to\n",
    "initialize the parameters of a multi-layer network.\n",
    "Our network will have multiple layers, each characterized by a weight\n",
    "matrix `W` and a bias vector `b`.\n",
    "We will use random initialization scaled by a small factor to ensure\n",
    "stable starting values for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "def init_params(scale, layer_sizes, rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weights and biases) for each layer in the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scale : float\n",
    "        A scaling factor to control the initial range of the weights.\n",
    "    layer_sizes : list of int\n",
    "        The sizes of each layer in the network.\n",
    "        e.g., [784, 1024, 1024, 10] means:\n",
    "            - Input layer: 784 units\n",
    "            - Hidden layer 1: 1024 units\n",
    "            - Hidden layer 2: 1024 units\n",
    "            - Output layer: 10 units\n",
    "    rng : numpy.random.RandomState\n",
    "        Random state for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : list of tuples (W, b)\n",
    "        Each tuple contains (W, b) for a layer.\n",
    "        - W is a (input_dim, output_dim) array of weights\n",
    "        - b is a (output_dim,) array of biases\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "        for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* We specify a list of layer sizes: for example,\n",
    "  `[784, 1024, 1024, 10]`.\n",
    "* For each pair of consecutive sizes `(m, n)`, we create a weight\n",
    "  matrix W of shape `(m, n)` and a bias vector `b` of shape `(n,)`.\n",
    "* Multiplying by scale ensures that initial values are not too large,\n",
    "  which helps prevent numerical issues early in training.\n",
    "* We store all `(W, b)` pairs in a list, one pair per layer, to be\n",
    "  used throughout training and inference.\n",
    "\n",
    "By calling `init_params(scale, layer_sizes)`, you obtain an\n",
    "easy-to-manipulate structure that keeps all the parameters needed for\n",
    "your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture and hyperparameters\n",
    "\n",
    "layer_sizes = [784, 1024, 1024, 10]  # 2 hidden layers\n",
    "param_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "\n",
    "params = init_params(param_scale, layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: count how many parameters are in your neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: Let's split the class into 2 groups.\n",
    "#          Group 1 makes their neural networks wider.\n",
    "#          Group 2 makes theirs deeper.\n",
    "#          We will compare the training and testing performance later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Forward Pass: The `predict` Function\n",
    "\n",
    "Once the network parameters are initialized, we need a function to\n",
    "perform the forward pass, producing an output for each batch of\n",
    "inputs.\n",
    "Below, we define `predict` to process data through multiple layers,\n",
    "using a `tanh` activation on the hidden layers, and compute a\n",
    "log-softmax on the final output layer for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def predict(params, inputs):\n",
    "    \"\"\"\n",
    "    Compute the network's output logits for a batch of inputs, then subtract\n",
    "    log-sum-exp for numerical stability (log-softmax).\n",
    "\n",
    "    Network architecture:\n",
    "      - Hidden layers use tanh activation\n",
    "      - Output layer is linear (we'll do log-softmax here)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        Network's parameters for each layer.\n",
    "    inputs : np.ndarray\n",
    "        A batch of input data of shape (batch_size, input_dim).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Log probabilities of shape (batch_size, 10).\n",
    "    \"\"\"\n",
    "    activations = inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(activations, w) + b\n",
    "        activations = np.tanh(outputs)\n",
    "\n",
    "    # Final layer (logits)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(activations, final_w) + final_b\n",
    "\n",
    "    # Log-Softmax: subtract logsumexp for numerical stability\n",
    "    return logits - logsumexp(logits, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "In the above function,\n",
    "* Hidden Layers (`tanh`):\n",
    "  Each hidden layer applies a linear transformation\n",
    "  (`np.dot(activations, w) + b`) followed by the hyperbolic tangent\n",
    "  activation function (`np.tanh`).\n",
    "* Final Layer (`logits`):\n",
    "  The last layer's output is not activated by tanh; instead, we use it\n",
    "  directly as logits.\n",
    "* Log-Softmax:\n",
    "  We transform logits to log probabilities by subtracting the\n",
    "  logsumexp(logits) along the class dimension.\n",
    "  This step ensures numerical stability and can be directly used to\n",
    "  compute losses like cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "To guide training, we need a loss function that measures how well our\n",
    "network's predictions match the true labels.\n",
    "This is like $\\chi^2$ when we need to fit a curve.\n",
    "Below, we define a function that computes the negative log-likelihood\n",
    "(NLL) over a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    Computes the average negative log-likelihood loss for a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray of shape (batch_size, 784)\n",
    "        - targets: np.ndarray of shape (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean negative log-likelihood over the batch.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    \n",
    "    # preds are log-probs, multiply with one-hot targets and sum -> log-likelihood\n",
    "    return -np.mean(np.sum(preds * targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "* Inputs and Targets:\n",
    "  A single batch typically consists of a set of input vectors (inputs)\n",
    "  and corresponding one-hot encoded labels (targets).\n",
    "* Forward Pass:\n",
    "  We call predict(params, inputs), which returns the log probabilities\n",
    "  for each class.\n",
    "* NLL Computation:\n",
    "  We multiply the log probabilities by the one-hot labels (so we only\n",
    "  pick out the log probability of the correct class for each example).\n",
    "  Summing these values (log-likelihood) and then negating yields the\n",
    "  negative log-likelihood.\n",
    "* Mean Value:\n",
    "  We take the average across the batch, yielding a scalar loss.\n",
    "\n",
    "This loss metric drives parameter updates: minimizing it pushes the\n",
    "network to assign higher probabilities to the correct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "\n",
    "While the network is trained by minimizing the negative log-likelihood\n",
    "(NLL), we often monitor accuracy to get an intuitive sense of model\n",
    "performance.\n",
    "The function below calculates the fraction of samples in a batch that\n",
    "are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "    \"\"\"\n",
    "    Computes classification accuracy of the network on a given batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The network's parameters.\n",
    "    batch : tuple (inputs, targets)\n",
    "        - inputs: np.ndarray (batch_size, 784)\n",
    "        - targets: np.ndarray (batch_size, 10) (one-hot labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "    target_class = np.argmax(targets, axis=1)  # ground truth index\n",
    "    predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "    return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "* Predicted Class:\n",
    "  * We use predict(params, inputs) to get log probabilities.\n",
    "  * Taking the argmax across the class dimension finds the class with\n",
    "    the highest log probability.\n",
    "* Compare to Ground Truth:\n",
    "  * We similarly get the ground truth label indices from the one-hot\n",
    "    targets by using np.argmax(targets, axis=1).\n",
    "* Accuracy Computation:\n",
    "  * We compute the fraction of instances where the predicted class\n",
    "    matches the ground-truth class.\n",
    "  * This value ranges between 0 (no correct predictions) and 1\n",
    "    (perfect classification).\n",
    "\n",
    "Monitoring accuracy alongside the loss offers a more intuitive measure\n",
    "of how well the model performs on a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Gradient Descent for Training: JIT-Compiled Update Function\n",
    "\n",
    "To optimize our network, we can use Stochastic Gradient Descent (SGD),\n",
    "updating parameters in the direction that reduces the loss.\n",
    "This is essentially the same algorithm we implemented in our\n",
    "optimization class!\n",
    "Except we only implement a single step for now.\n",
    "Here, we decorate our update step with `@jit` to compile it for\n",
    "efficient execution on CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "\n",
    "@jit\n",
    "def update(params, batch, step_size):\n",
    "    \"\"\"\n",
    "    Single step of gradient-based parameter update using simple SGD.\n",
    "\n",
    "    grad(loss)(params, batch) computes the gradient of the loss function\n",
    "    with respect to the parameters for the given batch.\n",
    "    \"\"\"\n",
    "    grads = grad(loss)(params, batch)\n",
    "    return [\n",
    "        (w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "* `grad(loss)` automatically differentiates the loss function with\n",
    "  respect to all parameters (`params`), i.e., parameter gradients.\n",
    "* SGD Update:\n",
    "  * For each weight `w`, we update it by `w - step_size * dw`.\n",
    "  * Similarly for each bias `b`.\n",
    "* `@jit` decorator:\n",
    "  * Compiles the update step using XLA (Accelerated Linear Algebra).\n",
    "  * Improves performance by running the update efficiently on CPU/GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Preparing the Batching Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # the number of samples per parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train   = train_images.shape[0]\n",
    "num_batches = num_train // batch_size\n",
    "\n",
    "def get_batch(rng=RandomState(0)):\n",
    "    \"\"\"\n",
    "    Generator function that yields shuffled batches indefinitely.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Randomly permute the indices\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i*batch_size : (i+1)*batch_size]\n",
    "            # Yield a tuple (inputs, labels) for this batch\n",
    "            yield (train_images[batch_idx], train_labels[batch_idx])\n",
    "\n",
    "train_batch_generator = get_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "* Shuffling:\n",
    "  Each epoch, we shuffle the training indices (`perm =\n",
    "  rng.permutation(num_train)`) to ensure that each mini-batch contains\n",
    "  a random subset of the dataset.\n",
    "* Batch Extraction:\n",
    "  We slice the permuted indices into chunks of size batch_size.\n",
    "  Each chunk defines which samples from train_images and train_labels\n",
    "  go into the current batch.\n",
    "\n",
    "By continuously yielding batches in an infinite `while True:` loop, we\n",
    "can keep calling `next(train_batch_generator)` without manually\n",
    "restarting the data pipeline each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "Now we can train our neural network by iterating over epochs and\n",
    "batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # the number of times we iterate over the entire training dataset.\n",
    "num_epochs    = 5      # the number of samples per parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time()\n",
    "\n",
    "    # Go through the entire training set\n",
    "    for _ in range(num_batches):\n",
    "        batch_data = next(train_batch_generator)\n",
    "        params     = update(params, batch_data, step_size=learning_rate)\n",
    "\n",
    "    epoch_time = time() - start_time\n",
    "\n",
    "    # Evaluate training and test accuracy\n",
    "    train_acc = accuracy(params, (train_images, train_labels))\n",
    "    test_acc  = accuracy(params, (test_images, test_labels))\n",
    "\n",
    "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
    "    print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy:     {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Breaking the above code down:\n",
    "\n",
    "* Epoch Loop:\n",
    "  We run `for epoch in range(num_epochs):` to repeat the training\n",
    "  process multiple times over the dataset.\n",
    "\n",
    "* Batch Loop:\n",
    "  For each epoch, we execute a loop `for _ in range(num_batches):` to\n",
    "  process all training batches.\n",
    "\n",
    "* Parameter Update:\n",
    "  * We call next(train_batch_generator) to obtain the next (inputs,\n",
    "    labels) batch.\n",
    "  * We then update the network parameters by calling:\n",
    "    `params = update(params, batch_data, step_size=learning_rate)`\n",
    "  * This performs a single SGD step, moving each parameter slightly\n",
    "    toward reducing the loss.\n",
    "  \n",
    "* Timing:\n",
    "  We measure how long each epoch takes by recording the start time\n",
    "  with `time()` and subtracting from the end time.\n",
    "\n",
    "* Evaluation:\n",
    "  After processing all batches for the epoch, we compute:\n",
    "  * `train_acc`:\n",
    "    Accuracy on the entire training set.\n",
    "  * `test_acc`:\n",
    "    Accuracy on the reserved test set.\n",
    "  \n",
    "* Logging:\n",
    "  We print out the epoch number, epoch duration, and both training and\n",
    "  test accuracies.\n",
    "  Monitoring test accuracy helps assess how well the model generalizes\n",
    "  beyond the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: At this point, we have a fully operational training\n",
    "#          pipeline for MNIST.\n",
    "#          You can experiment with different hyperparameters (e.g.,\n",
    "#          learning rate, batch size, number of epochs, network\n",
    "#          architecture) to see how they affect model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing a Custom Image\n",
    "\n",
    "To run inference on your own handwriting, we first need to load the\n",
    "image from disk and convert it into a format suitable for our trained\n",
    "network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_image(file):\n",
    "    \"\"\"\n",
    "    Loads a grayscale image from `file`, resizes it to 28x28,\n",
    "    and converts it to a (784,) float32 array with values in [0, 1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to the image file (e.g., a PNG or JPG).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of shape (1, 784) containing normalized pixel values\n",
    "        suitable as input to our trained model.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale and resize to 28x28\n",
    "    img = Image.open(file).convert('L').resize((28, 28))\n",
    "\n",
    "    # Convert to a NumPy array and normalize pixel intensities to [0, 1]\n",
    "    arr = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "    # Flatten the 28x28 image into a single 784-dimensional vector\n",
    "    arr = arr.flatten()\n",
    "\n",
    "    # Reshape to (1, 784) to match the model's expected input batch shape\n",
    "    return np.array([arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Predicting the Digit Class\n",
    "\n",
    "With a properly formatted image, we can classify it using our trained\n",
    "model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(params, file_path):\n",
    "    \"\"\"\n",
    "    Predict the digit class for a custom handwritten image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of (W, b) tuples\n",
    "        The trained network parameters.\n",
    "    file_path : str\n",
    "        Path to the custom image file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The predicted digit label (0 through 9).\n",
    "    \"\"\"\n",
    "    # Convert the image to a suitable NumPy array\n",
    "    arr_np = load_image(file_path)\n",
    "    \n",
    "    # Use our 'predict' function to get log probabilities for each class\n",
    "    log_probs = predict(params, arr_np)  # shape: (1, 10)\n",
    "    \n",
    "    # Select the digit class with the highest log probability\n",
    "    return int(np.argmax(log_probs, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sample image from GitHub\n",
    "\n",
    "! wget https://raw.githubusercontent.com/rndsrc/stat2ml/refs/heads/main/fig/sample.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict what the digit is\n",
    "\n",
    "label = predict_digit(params, \"sample.png\")\n",
    "print(f\"Predicted digit: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDSON: 1. Capture or scan your handwritten digit and save as\n",
    "#             \"sample.png\".\n",
    "#             You may also use your cell phone for this.\n",
    "#\n",
    "#          2. Call the function:\n",
    "#\n",
    "#                 label = predict_digit(params, \"sample.png\")\n",
    "#                 print(f\"Predicted digit: {label}\")\n",
    "#\n",
    "#          3. Inspect the result:\n",
    "#             See whether the predicted label matches the actual digit\n",
    "#             you wrote.\n",
    "#\n",
    "#             With these two functions, your MNIST-trained model can\n",
    "#             be used in real-world testing scenarios, allowing you to\n",
    "#             evaluate its performance on custom, hand-drawn images.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
